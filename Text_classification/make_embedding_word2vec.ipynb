{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# 1 .Tutorial from https://www.tensorflow.org/tutorials/text/word2vec\n",
    "# 2. http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "# 3. https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "# 4. https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ns = 4 # Number of negative samples per target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step generating dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['данное', 'устройство', 'уже', 'было', 'ранее', 'зарегистрированно', 'в', 'нашей', 'сети']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Данное устройство уже было ранее зарегистрированно в нашей сети\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make vocabulary and inverse vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'<pad>': 0, 'данное': 1, 'устройство': 2, 'уже': 3, 'было': 4, 'ранее': 5, 'зарегистрированно': 6, 'в': 7, 'нашей': 8, 'сети': 9}\n",
      "inverse_vocab: {0: '<pad>', 1: 'данное', 2: 'устройство', 3: 'уже', 4: 'было', 5: 'ранее', 6: 'зарегистрированно', 7: 'в', 8: 'нашей', 9: 'сети'}\n"
     ]
    }
   ],
   "source": [
    "vocab, inverse_vocab = {'<pad>': 0}, {0: '<pad>'}\n",
    "for i, token in enumerate(tokens):\n",
    "    vocab[token] = vocab.get(token, i+1)\n",
    "    inverse_vocab[i+1] = token\n",
    "vocab_size = len(vocab)\n",
    "print(\"vocab:\", vocab)\n",
    "print(\"inverse_vocab:\", inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating positive skip-grams - pairs [target word, context word]. Context word is taken \n",
    "# from the window of size - window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4], [4, 2], [8, 6], [2, 4], [4, 6], [7, 9], [6, 4], [8, 7], [2, 1], [7, 8], [3, 1], [9, 7], [2, 3], [5, 3], [7, 5], [4, 5], [5, 7], [1, 3], [3, 5], [4, 3], [8, 9], [3, 2], [1, 2], [9, 8], [7, 6], [6, 8], [5, 4], [6, 5], [5, 6], [6, 7]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(positive_skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4] уже было\n",
      "[4, 2] было устройство\n",
      "[8, 6] нашей зарегистрированно\n",
      "[2, 4] устройство было\n",
      "[4, 6] было зарегистрированно\n"
     ]
    }
   ],
   "source": [
    "for skip_gram in positive_skip_grams[:5]:\n",
    "    print(f\"{skip_gram} {inverse_vocab[skip_gram[0]]} {inverse_vocab[skip_gram[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n",
      "tf.Tensor([[3]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "print(target_word, context_word)\n",
    "print(context_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative skip-gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 было уже\n",
      "tf.Tensor([2 9 0 4], shape=(4,), dtype=int64)\n",
      "['устройство', 'сети', '<pad>', 'было']\n"
     ]
    }
   ],
   "source": [
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,\n",
    "    num_true=1,\n",
    "    num_sampled=num_ns,\n",
    "    unique=True,\n",
    "    range_max=vocab_size\n",
    ")\n",
    "print(target_word, context_word, inverse_vocab[target_word], inverse_vocab[context_word])\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate positive class (right context word) with negative class (wrong context word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [4]], shape=(5, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lable tensor and flattern all tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor([3 2 9 0 4], shape=(5,), dtype=int64)\n",
      "tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)\n",
    "print(target)\n",
    "print(context)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence,\n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "#         print(positive_skip_grams)\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            \n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=num_ns,\n",
    "              unique=True,\n",
    "              range_max=vocab_size)\n",
    "\n",
    "          # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "              # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply created function to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"data/processing_phrases_more_one_word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and number of words in a sequence. Number of words in a sequence set to the \n",
    "# third quartile of senteneces length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "sequence_length = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text vectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes a vocabulary and create inverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'вы', 'в', 'на', 'я', 'не', 'за', 'ваш', 'и', 'мочь', 'по', 'у', 'роутер', 'интернет', 'помочь', 'обращение', 'с', 'быть', 'наш']\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749130\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118  31 464  71  23  39] => ['подсказать', 'как', 'добавить', 'устройство', 'к', 'договор']\n",
      "[  17  182   17    2 1032  119] => ['с', 'который', 'с', 'вы', 'общаться', 'да']\n",
      "[312 168   0   0   0   0] => ['ноутбук', 'возможно', '', '', '', '']\n",
      "[433  17  60   3 108  39] => ['заходить', 'с', 'он', 'в', 'кабинет', 'договор']\n",
      "[385   0   0   0   0   0] => ['мбс', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate targets, contexts and labels by created function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 749130/749130 [01:12<00:00, 10349.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686914 686914 686914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size)\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class inheriting from Model and implementing NN for train word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "        self.dots = Dot(axes=(3, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = [64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_dim value:  64\n",
      "Epoch 1/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 1.4914 - accuracy: 0.4214\n",
      "Epoch 2/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 1.0603 - accuracy: 0.6038\n",
      "Epoch 3/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.9054 - accuracy: 0.6682\n",
      "Epoch 4/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.8173 - accuracy: 0.7008\n",
      "Epoch 5/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.7560 - accuracy: 0.7244\n",
      "Epoch 6/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.7087 - accuracy: 0.7421\n",
      "Epoch 7/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.6702 - accuracy: 0.7571\n",
      "Epoch 8/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.6379 - accuracy: 0.7698\n",
      "Epoch 9/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.6101 - accuracy: 0.7801\n",
      "Epoch 10/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.5859 - accuracy: 0.7893\n",
      "Epoch 11/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.5647 - accuracy: 0.7976\n",
      "Epoch 12/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.5459 - accuracy: 0.8048\n",
      "Epoch 13/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.5292 - accuracy: 0.8115\n",
      "Epoch 14/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.5143 - accuracy: 0.8174\n",
      "Epoch 15/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.5009 - accuracy: 0.8223\n",
      "Epoch 16/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.4888 - accuracy: 0.8266\n",
      "Epoch 17/20\n",
      "670/670 [==============================] - 6s 10ms/step - loss: 0.4780 - accuracy: 0.8305\n",
      "Epoch 18/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.4681 - accuracy: 0.8340\n",
      "Epoch 19/20\n",
      "670/670 [==============================] - 7s 10ms/step - loss: 0.4591 - accuracy: 0.8371\n",
      "Epoch 20/20\n",
      "670/670 [==============================] - 6s 9ms/step - loss: 0.4509 - accuracy: 0.8402\n",
      "Evaluate\n",
      "670/670 [==============================] - 1s 1ms/step - loss: 0.4232 - accuracy: 0.8520\n",
      "embedding_dim value:  128\n",
      "Epoch 1/20\n",
      "670/670 [==============================] - 12s 17ms/step - loss: 1.4529 - accuracy: 0.4496\n",
      "Epoch 2/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.9760 - accuracy: 0.6423\n",
      "Epoch 3/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.8172 - accuracy: 0.7046\n",
      "Epoch 4/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.7260 - accuracy: 0.7386\n",
      "Epoch 5/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.6606 - accuracy: 0.7636\n",
      "Epoch 6/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.6093 - accuracy: 0.7831\n",
      "Epoch 7/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.5673 - accuracy: 0.7996\n",
      "Epoch 8/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.5319 - accuracy: 0.8132\n",
      "Epoch 9/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.5019 - accuracy: 0.8246\n",
      "Epoch 10/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.4762 - accuracy: 0.8343\n",
      "Epoch 11/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.4539 - accuracy: 0.8423\n",
      "Epoch 12/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.4345 - accuracy: 0.8496\n",
      "Epoch 13/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.4176 - accuracy: 0.8556\n",
      "Epoch 14/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.4026 - accuracy: 0.8607\n",
      "Epoch 15/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3894 - accuracy: 0.8651\n",
      "Epoch 16/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3777 - accuracy: 0.8690\n",
      "Epoch 17/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3671 - accuracy: 0.8724\n",
      "Epoch 18/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3577 - accuracy: 0.8755\n",
      "Epoch 19/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3493 - accuracy: 0.8783\n",
      "Epoch 20/20\n",
      "670/670 [==============================] - 11s 16ms/step - loss: 0.3416 - accuracy: 0.8805\n",
      "Evaluate\n",
      "670/670 [==============================] - 1s 2ms/step - loss: 0.3153 - accuracy: 0.8924\n",
      "embedding_dim value:  256\n",
      "Epoch 1/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 1.4076 - accuracy: 0.4739\n",
      "Epoch 2/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.8933 - accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.7303 - accuracy: 0.7405\n",
      "Epoch 4/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.6341 - accuracy: 0.7765\n",
      "Epoch 5/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.5644 - accuracy: 0.8037\n",
      "Epoch 6/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.5104 - accuracy: 0.8240\n",
      "Epoch 7/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.4676 - accuracy: 0.8403\n",
      "Epoch 8/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.4332 - accuracy: 0.8527\n",
      "Epoch 9/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.4052 - accuracy: 0.8624\n",
      "Epoch 10/20\n",
      "670/670 [==============================] - 21s 32ms/step - loss: 0.3822 - accuracy: 0.8698\n",
      "Epoch 11/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.3630 - accuracy: 0.8756\n",
      "Epoch 12/20\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3470 - accuracy: 0.8808\n",
      "Epoch 13/20\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3335 - accuracy: 0.8850\n",
      "Epoch 14/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.3220 - accuracy: 0.8885\n",
      "Epoch 15/20\n",
      "670/670 [==============================] - 21s 31ms/step - loss: 0.3122 - accuracy: 0.8912\n",
      "Epoch 16/20\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3038 - accuracy: 0.8933\n",
      "Epoch 17/20\n",
      "670/670 [==============================] - 23s 34ms/step - loss: 0.2967 - accuracy: 0.8951\n",
      "Epoch 18/20\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2905 - accuracy: 0.8964\n",
      "Epoch 19/20\n",
      "670/670 [==============================] - 20s 30ms/step - loss: 0.2852 - accuracy: 0.8976\n",
      "Epoch 20/20\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2805 - accuracy: 0.8985\n",
      "Evaluate\n",
      "670/670 [==============================] - 2s 2ms/step - loss: 0.2536 - accuracy: 0.9115\n",
      "embedding_dim value:  512\n",
      "Epoch 1/20\n",
      "670/670 [==============================] - 45s 67ms/step - loss: 1.3560 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "670/670 [==============================] - 44s 66ms/step - loss: 0.8179 - accuracy: 0.7097\n",
      "Epoch 3/20\n",
      "670/670 [==============================] - 45s 67ms/step - loss: 0.6485 - accuracy: 0.7730\n",
      "Epoch 4/20\n",
      "670/670 [==============================] - 42s 63ms/step - loss: 0.5456 - accuracy: 0.8128\n",
      "Epoch 5/20\n",
      "670/670 [==============================] - 44s 65ms/step - loss: 0.4737 - accuracy: 0.8395\n",
      "Epoch 6/20\n",
      "670/670 [==============================] - 42s 63ms/step - loss: 0.4221 - accuracy: 0.8577\n",
      "Epoch 7/20\n",
      "670/670 [==============================] - 43s 64ms/step - loss: 0.3844 - accuracy: 0.8701\n",
      "Epoch 8/20\n",
      "670/670 [==============================] - 42s 63ms/step - loss: 0.3564 - accuracy: 0.8787\n",
      "Epoch 9/20\n",
      "670/670 [==============================] - 43s 64ms/step - loss: 0.3352 - accuracy: 0.8846\n",
      "Epoch 10/20\n",
      "670/670 [==============================] - 44s 66ms/step - loss: 0.3190 - accuracy: 0.8888\n",
      "Epoch 11/20\n",
      "670/670 [==============================] - 69s 103ms/step - loss: 0.3065 - accuracy: 0.8918\n",
      "Epoch 12/20\n",
      "670/670 [==============================] - 136s 203ms/step - loss: 0.2968 - accuracy: 0.8940\n",
      "Epoch 13/20\n",
      "670/670 [==============================] - 45s 67ms/step - loss: 0.2891 - accuracy: 0.8955\n",
      "Epoch 14/20\n",
      "670/670 [==============================] - 46s 68ms/step - loss: 0.2830 - accuracy: 0.8964\n",
      "Epoch 15/20\n",
      "670/670 [==============================] - 45s 67ms/step - loss: 0.2781 - accuracy: 0.8970\n",
      "Epoch 16/20\n",
      "670/670 [==============================] - 46s 68ms/step - loss: 0.2741 - accuracy: 0.8976\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - 46s 69ms/step - loss: 0.2708 - accuracy: 0.8980\n",
      "Epoch 18/20\n",
      "670/670 [==============================] - 45s 67ms/step - loss: 0.2681 - accuracy: 0.8983\n",
      "Epoch 19/20\n",
      "670/670 [==============================] - 45s 68ms/step - loss: 0.2658 - accuracy: 0.8985\n",
      "Epoch 20/20\n",
      "670/670 [==============================] - 46s 69ms/step - loss: 0.2639 - accuracy: 0.8986\n",
      "Evaluate\n",
      "670/670 [==============================] - 5s 4ms/step - loss: 0.2327 - accuracy: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{64: 0.8519618511199951,\n",
       " 128: 0.892391562461853,\n",
       " 256: 0.9115248918533325,\n",
       " 512: 0.915403425693512}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for embedding_dim in embedding_dims:\n",
    "    print(\"embedding_dim value: \", embedding_dim)\n",
    "    w2v = Word2Vec(vocab_size, embedding_dim)\n",
    "    w2v.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    w2v.fit(dataset, epochs=20)\n",
    "    print(\"Evaluate\")\n",
    "    _, results[embedding_dim] = w2v.evaluate(dataset)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "670/670 [==============================] - 27s 39ms/step - loss: 1.4059 - accuracy: 0.4754\n",
      "Epoch 2/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.8951 - accuracy: 0.6778\n",
      "Epoch 3/40\n",
      "670/670 [==============================] - 22s 34ms/step - loss: 0.7323 - accuracy: 0.7392\n",
      "Epoch 4/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.6357 - accuracy: 0.7760\n",
      "Epoch 5/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.5655 - accuracy: 0.8031\n",
      "Epoch 6/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.5113 - accuracy: 0.8236\n",
      "Epoch 7/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.4682 - accuracy: 0.8399\n",
      "Epoch 8/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.4336 - accuracy: 0.8524\n",
      "Epoch 9/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.4055 - accuracy: 0.8621\n",
      "Epoch 10/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3823 - accuracy: 0.8700\n",
      "Epoch 11/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3631 - accuracy: 0.8761\n",
      "Epoch 12/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.3470 - accuracy: 0.8812\n",
      "Epoch 13/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3334 - accuracy: 0.8853\n",
      "Epoch 14/40\n",
      "670/670 [==============================] - 22s 34ms/step - loss: 0.3220 - accuracy: 0.8886\n",
      "Epoch 15/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3122 - accuracy: 0.8913\n",
      "Epoch 16/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.3038 - accuracy: 0.8934\n",
      "Epoch 17/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2966 - accuracy: 0.8952\n",
      "Epoch 18/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2904 - accuracy: 0.8966\n",
      "Epoch 19/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2851 - accuracy: 0.8977\n",
      "Epoch 20/40\n",
      "670/670 [==============================] - 21s 32ms/step - loss: 0.2805 - accuracy: 0.8988\n",
      "Epoch 21/40\n",
      "670/670 [==============================] - 21s 32ms/step - loss: 0.2764 - accuracy: 0.8996\n",
      "Epoch 22/40\n",
      "670/670 [==============================] - 24s 36ms/step - loss: 0.2729 - accuracy: 0.9002\n",
      "Epoch 23/40\n",
      "670/670 [==============================] - 21s 31ms/step - loss: 0.2698 - accuracy: 0.9007\n",
      "Epoch 24/40\n",
      "670/670 [==============================] - 21s 32ms/step - loss: 0.2671 - accuracy: 0.9011\n",
      "Epoch 25/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2647 - accuracy: 0.9015\n",
      "Epoch 26/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2626 - accuracy: 0.9018\n",
      "Epoch 27/40\n",
      "670/670 [==============================] - 23s 34ms/step - loss: 0.2607 - accuracy: 0.9020\n",
      "Epoch 28/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2590 - accuracy: 0.9022\n",
      "Epoch 29/40\n",
      "670/670 [==============================] - 23s 34ms/step - loss: 0.2575 - accuracy: 0.9023\n",
      "Epoch 30/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2561 - accuracy: 0.9025\n",
      "Epoch 31/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2549 - accuracy: 0.9026\n",
      "Epoch 32/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2538 - accuracy: 0.9027\n",
      "Epoch 33/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2528 - accuracy: 0.9028\n",
      "Epoch 34/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2518 - accuracy: 0.9029\n",
      "Epoch 35/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2510 - accuracy: 0.9029\n",
      "Epoch 36/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2502 - accuracy: 0.9030\n",
      "Epoch 37/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2495 - accuracy: 0.9031\n",
      "Epoch 38/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2489 - accuracy: 0.9031\n",
      "Epoch 39/40\n",
      "670/670 [==============================] - 22s 33ms/step - loss: 0.2483 - accuracy: 0.9032\n",
      "Epoch 40/40\n",
      "670/670 [==============================] - 22s 32ms/step - loss: 0.2478 - accuracy: 0.9032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f473920b1f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embedding vectors to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors_norm_sentence_length_6_256_v2.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_norm_sentence_length_6_256_v2.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
