{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakhterov/ML_projects/blob/master/Text_classification/make_embedding_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leewtJyC1cU8",
        "outputId": "ebee0cec-4c5f-4163-ae35-a3dc25370cf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8QfQxtYcvUV2"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3BRYWwtfvUV6"
      },
      "outputs": [],
      "source": [
        "# Reference\n",
        "# 1 .Tutorial from https://www.tensorflow.org/tutorials/text/word2vec\n",
        "# 2. http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "# 3. https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
        "# 4. https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30\n",
        "# 5. https://colab.research.google.com/drive/1vZuJSHulZhn7ihoPWwVQGic-fGQCtamX#scrollTo=lqPFJRv28lHC (word2vec_train.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5OGMrYM4vUV7"
      },
      "outputs": [],
      "source": [
        "num_ns = 4 # Number of negative samples per target word\n",
        "window_size = 2 # Size of sampling windows\n",
        "vocab_size = 10000 # Vocabulary size\n",
        "sequence_length = 6 # Number of words in a sequence set to the third quartile of senteneces length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size):\n",
        "    targets, contexts, labels = [], [], []\n",
        "\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "    for sequence in tqdm.tqdm(sequences):\n",
        "      skip_grams, sg_labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=num_ns)\n",
        "\n",
        "      positive_skip_grams = [skip_gram for idx, skip_gram in enumerate(skip_grams) if sg_labels[idx]==1]\n",
        "      negative_skip_grams = [skip_gram for idx, skip_gram in enumerate(skip_grams) if sg_labels[idx]==0]\n",
        "\n",
        "      for target_word, context_word in positive_skip_grams:\n",
        "        negative_skip_grams_candidates = [skip_gram for skip_gram in negative_skip_grams if\n",
        "                                          skip_gram[0]==target_word]\n",
        "        current_negative_skip_grams = negative_skip_grams_candidates[:num_ns]\n",
        "        for skip_gram in current_negative_skip_grams:\n",
        "          negative_skip_grams.remove(skip_gram)\n",
        "\n",
        "        context = tf.squeeze([context_word] + [x[1] for x in current_negative_skip_grams] )\n",
        "        label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "        targets.append(target_word)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "\n",
        "    return targets, contexts, labels"
      ],
      "metadata": {
        "id": "Qb_0TOMhaa0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get preprocessed data"
      ],
      "metadata": {
        "id": "iqzfdL3AWMTW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n-Ni6sK_vUWE"
      },
      "outputs": [],
      "source": [
        "text_ds = tf.data.TextLineDataset(\"/content/drive/MyDrive/Colab Notebooks/Data/processing_phrases_more_one_word.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UlT5vs-kvUWE"
      },
      "outputs": [],
      "source": [
        "# Use the text vectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G93AiP4wvUWE"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_MXXZqaavUWE"
      },
      "outputs": [],
      "source": [
        "# Computes a vocabulary and create inverse_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y83GqfjuvUWF",
        "outputId": "1a2957ae-ee4b-4228-de40-eace9d9a7a55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'вы', 'в', 'на', 'я', 'не', 'за', 'ваш', 'и', 'мочь', 'по', 'у', 'роутер', 'интернет', 'помочь', 'обращение', 'с', 'быть', 'наш']\n"
          ]
        }
      ],
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i3BNzu_SvUWF"
      },
      "outputs": [],
      "source": [
        "text_vector_ds = text_ds.batch(1024).prefetch(buffer_size=tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OX1gv6HovUWF",
        "outputId": "005e976f-7da3-4d77-ccae-3c146f88deed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "749130\n"
          ]
        }
      ],
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YzcnnhrUvUWF",
        "outputId": "1f662cf1-a9a3-4398-b89c-eb3fe11681b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[118  31 464  71  23  39] => ['подсказать', 'как', 'добавить', 'устройство', 'к', 'договор']\n",
            "[  17  182   17    2 1032  119] => ['с', 'который', 'с', 'вы', 'общаться', 'да']\n",
            "[312 168   0   0   0   0] => ['ноутбук', 'возможно', '', '', '', '']\n",
            "[433  17  60   3 108  39] => ['заходить', 'с', 'он', 'в', 'кабинет', 'договор']\n",
            "[385   0   0   0   0   0] => ['мбс', '', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences[:5]:\n",
        "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QX6uOSNEvUWG"
      },
      "outputs": [],
      "source": [
        "# Generate targets, contexts and labels by created function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9OpHYAMevUWG",
        "outputId": "d30c79fc-8214-4f2b-af9c-a68e6861e877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 749130/749130 [03:40<00:00, 3403.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "687552 687552 687552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size)\n",
        "print(len(targets), len(contexts), len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30_BNqENLUfS",
        "outputId": "7eac86b8-cf1e-4604-f80c-4918026707b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (687552,)\n",
            "contexts.shape: (687552, 5)\n",
            "labels.shape: (687552, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vhXU7arMvUWG",
        "outputId": "6f97b63d-673c-4712-cfc7-48eb428f176a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int32, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GbWpU5TavUWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f101539-b55a-4dae-fc76-15416fd0b131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int32, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QDcYF7iYvUWH"
      },
      "outputs": [],
      "source": [
        "# Make class inheriting from Model and implementing NN for train word2vec embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "riJ7fHZWvUWH"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = Embedding(vocab_size,\n",
        "                                          embedding_dim,\n",
        "                                          input_length=1,\n",
        "                                          name=\"w2v_embedding\")\n",
        "        self.context_embedding = Embedding(vocab_size,\n",
        "                                           embedding_dim,\n",
        "                                           input_length=num_ns+1)\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        we = self.target_embedding(target)\n",
        "        ce = self.context_embedding(context)\n",
        "        dots = tf.einsum('be,bce->bc', we, ce)\n",
        "        return dots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uSX73vvCvUWH"
      },
      "outputs": [],
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6np7mfAWvUWH"
      },
      "outputs": [],
      "source": [
        "# Select embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KXmpmEX0vUWR"
      },
      "outputs": [],
      "source": [
        "# embedding_dims = [64, 128, 256, 512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ooWQCHuWvUWR"
      },
      "outputs": [],
      "source": [
        "# results = {}\n",
        "# for embedding_dim in embedding_dims:\n",
        "#     print(\"embedding_dim value: \", embedding_dim)\n",
        "#     w2v = Word2Vec(vocab_size, embedding_dim)\n",
        "#     w2v.compile(optimizer='adam',\n",
        "#                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "#                 metrics=['accuracy'])\n",
        "#     w2v.fit(dataset, epochs=20)\n",
        "#     print(\"Evaluate\")\n",
        "#     _, results[embedding_dim] = w2v.evaluate(dataset)\n",
        "# results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2To63W-HvUWS"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "an8zfp6AvUWS"
      },
      "outputs": [],
      "source": [
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BJBzkUv_vUWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3193e1-5bce-4a81-d2f2-db66487ea884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "671/671 [==============================] - 50s 73ms/step - loss: 0.6556 - accuracy: 0.8427\n",
            "Epoch 2/10\n",
            "671/671 [==============================] - 49s 73ms/step - loss: 0.2357 - accuracy: 0.9243\n",
            "Epoch 3/10\n",
            "671/671 [==============================] - 48s 72ms/step - loss: 0.1765 - accuracy: 0.9421\n",
            "Epoch 4/10\n",
            "671/671 [==============================] - 49s 72ms/step - loss: 0.1321 - accuracy: 0.9580\n",
            "Epoch 5/10\n",
            "671/671 [==============================] - 49s 74ms/step - loss: 0.0955 - accuracy: 0.9722\n",
            "Epoch 6/10\n",
            "671/671 [==============================] - 49s 73ms/step - loss: 0.0665 - accuracy: 0.9836\n",
            "Epoch 7/10\n",
            "671/671 [==============================] - 50s 74ms/step - loss: 0.0452 - accuracy: 0.9912\n",
            "Epoch 8/10\n",
            "671/671 [==============================] - 50s 74ms/step - loss: 0.0306 - accuracy: 0.9952\n",
            "Epoch 9/10\n",
            "671/671 [==============================] - 50s 74ms/step - loss: 0.0210 - accuracy: 0.9973\n",
            "Epoch 10/10\n",
            "671/671 [==============================] - 50s 74ms/step - loss: 0.0149 - accuracy: 0.9984\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4361f93ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "word2vec.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "waC0voH3vUWS"
      },
      "outputs": [],
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND7TIk0YvUWS"
      },
      "outputs": [],
      "source": [
        "# save embedding vectors to the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOhhLOTsvUWT"
      },
      "outputs": [],
      "source": [
        "out_v = io.open('vectors_norm_sentence_length_6_256_v2.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata_norm_sentence_length_6_256_v2.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "    if index == 0:\n",
        "        continue  # skip 0, it's padding.\n",
        "    vec = weights[index]\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "    out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUsczze8vUWT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}