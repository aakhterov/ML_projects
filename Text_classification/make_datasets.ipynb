{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dialogs.json\", mode=\"r\") as file:\n",
    "    dialogs_json = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversation themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'нет оценки',\n",
      " 236486: 'Заявка на подключение Интернет',\n",
      " 236487: 'Консультация по подключению Интернет',\n",
      " 236488: 'Консультация по КТВ',\n",
      " 236489: 'Финансовая консультация',\n",
      " 236490: 'Консультация по интерактивному телевидению',\n",
      " 236491: 'Не работает ЛК приложение – рекомендация по работе',\n",
      " 245949: 'ТТ Финансы',\n",
      " 245950: 'ТТ Клиентские',\n",
      " 245951: 'Общая проблема - информирование клиента',\n",
      " 245952: 'Не работает интернет – рекомендация по настройке',\n",
      " 245953: 'Смена тарифного плана',\n",
      " 245954: 'Инсталляция',\n",
      " 245955: 'Консультация по доп.услугам',\n",
      " 245956: 'Прочее',\n",
      " 246249: 'ТТ Телевидение',\n",
      " 246250: 'Не работает КТВ – рекомендация по настройке',\n",
      " 246251: 'Не работает Интерактивное ТВ – рекомендация по настройке',\n",
      " 246252: 'Не работают прочие услуги – рекомендация по работе',\n",
      " 247714: 'Возврат ДС/оборудования // Переоформление /расторжение договора'}\n"
     ]
    }
   ],
   "source": [
    "er = {}\n",
    "with open(\"data/dialogs_table_employee_remarks.sql\", mode=\"r\") as file:\n",
    "    for line in file:\n",
    "        if line[0] == '(':\n",
    "            rec = line[1:-3].split(',')\n",
    "            er[int(rec[0])] = rec[1].strip(\" '\").replace(\"\\\\\",\"\").replace(\"\\\"\",\"\")\n",
    "pprint(er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping conversation topics to group related topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{236486: 'connect_to_inet',\n",
      " 236487: 'connect_to_inet',\n",
      " 236488: 'KTV',\n",
      " 236489: 'finance',\n",
      " 236490: 'ITV',\n",
      " 236491: 'no_LK',\n",
      " 245949: 'finance',\n",
      " 245950: 'low_speed',\n",
      " 245952: 'no_internet',\n",
      " 245953: 'tariff_changes',\n",
      " 245954: 'install',\n",
      " 245955: 'additional',\n",
      " 246249: 'TV',\n",
      " 246250: 'no_KTV',\n",
      " 246251: 'no_ITV',\n",
      " 247714: 'termination_return'}\n"
     ]
    }
   ],
   "source": [
    "categories_to_er = {\n",
    "    \"connect_to_inet\": [236486, 236487],\n",
    "    \"install\": [245954],\n",
    "    \"finance\": [236489, 245949],\n",
    "    \"no_LK\": [236491],\n",
    "    \"no_ITV\": [246251],\n",
    "    \"no_KTV\": [246250],\n",
    "    \"low_speed\": [245950],\n",
    "    \"no_internet\": [245952],\n",
    "    \"termination_return\": [247714],\n",
    "    \"tariff_changes\": [245953],\n",
    "    \"TV\": [246249],\n",
    "    \"KTV\": [236488],\n",
    "    \"ITV\": [236490],\n",
    "    \"additional\": [245955],\n",
    "#     \"other\": [245953, 246249, 236488, 236490, 245955]\n",
    "}\n",
    "er_to_categories = {}\n",
    "for category, value in categories_to_er.items():\n",
    "    er_to_categories = {**er_to_categories, **{er: category for er in value}}\n",
    "pprint(er_to_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files with conversations by themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {}\n",
    "for _, value in dialogs_json.items():\n",
    "    if value[\"er\"] in er_to_categories:\n",
    "        category = er_to_categories[value[\"er\"]]\n",
    "        with open(f\"data/{category}.csv\", mode=\"a\") as file:\n",
    "            phrases = \" \".join(value[\"visitors_phrases\"][:2])\n",
    "            print(f\"1;{phrases};\", file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation example. visitors_phrases - customer phrases, agents_phrases - employee phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'er': 236488,\n",
       " 'site': 159859,\n",
       " 'created_at': '2020-04-30 21:57:43',\n",
       " 'visitors_phrases': ['Доьрый вечер',\n",
       "  'Мне нужно отключить кабельное телевидение. Я пользуюсь только интернетом',\n",
       "  'Г. Одинцово',\n",
       "  'Подъезд 1',\n",
       "  'Андреева Маргарита Николаевна',\n",
       "  'Хорошо. Эту справку нужно отнести в ук?',\n",
       "  'Нет'],\n",
       " 'agents_phrases': ['Добрый вечер!',\n",
       "  'Какой у вас вопрос?',\n",
       "  'Уточните адрес (улица',\n",
       "  'Для составления заявки',\n",
       "  'Информацию зафиксировали. К вам будет направлен сотрудник',\n",
       "  'Да можете сами направить',\n",
       "  'Возможно чем-то еще могу вам помочь?',\n",
       "  'Спасибо за Ваше обращение в нашу компанию. Всегда рады Вам помочь. Хорошего вечера!']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_json[\"50988\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all phrases to one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = []\n",
    "for _, value in dialogs_json.items():\n",
    "    phrases = value[\"visitors_phrases\"] + value[\"agents_phrases\"]\n",
    "    all_phrases += phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/all_phrases.txt\", mode=\"w\") as file:\n",
    "    for phrase in all_phrases:\n",
    "        print(phrase, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the words associated with wi-fi to the same form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wifi_union = [(\"wi\", \"fi\"), (\"вай\", \"фай\")]\n",
    "wifi_naming = {\"wifi\": \"вайфай\", \"wi-fi\": \"вайфай\", \"вайфая\": \"вайфай\"}\n",
    "def wifi_preprocessing(phrase: str) -> str:   \n",
    "    words = [word.replace(\"фая\", \"фай\") for word in phrase.split()]\n",
    "    i = 0\n",
    "    united_words = []\n",
    "    while i<len(words):\n",
    "        for pair in wifi_union:\n",
    "            if i < len(words)-1 and words[i] == pair[0] and words[i+1] == pair[1]:\n",
    "                united_words.append(words[i] + words[i+1])\n",
    "                i += 2\n",
    "                break\n",
    "        else:\n",
    "            united_words.append(words[i])\n",
    "            i += 1\n",
    "    words = [word if word not in wifi_naming else wifi_naming[word] for word in united_words]\n",
    "    result_phrase = \" \".join(words)\n",
    "    return result_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text: str) -> str:  \n",
    "    return re.sub(r'(www|http:|https:)+[^\\s]+[\\w]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all expected digits, russian letters, space, ., ! and ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct_and_latin(text: str) -> str:  \n",
    "    return re.sub(r'[^0-9А-ЯЁа-яё\\s.!?]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace digits by symbol #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_digits(text: str) -> str:\n",
    "    for length in range(10,0,-1):\n",
    "        pattern = r'\\d{' + str(length) + '}'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            text = re.sub(pattern, '#'*length, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting words to normal form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_words = ['ловит', 'ловита', 'ловиту']\n",
    "def normalize(text: str) -> str:\n",
    "    normilized_sentence = []\n",
    "    for word in text.split():\n",
    "        if word not in exception_words:\n",
    "            word_forms = morph.parse(word)\n",
    "            if word_forms:\n",
    "                normilized_sentence.append(word_forms[0].normal_form)\n",
    "        else:\n",
    "            normilized_sentence.append(word)\n",
    "    return \" \".join(normilized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split phrase by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(separators, phrases):   \n",
    "    tmp = phrases\n",
    "    for sep in separators:\n",
    "        result = []\n",
    "        for phrase in list(map(lambda x: x.split(sep), tmp)):\n",
    "            result += phrase\n",
    "        tmp = result\n",
    "    return [phrase.strip() for phrase in result if phrase.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text: str) -> str:\n",
    "    stop_words = ['здравствуйте', 'добрый день', 'доброе утро', 'доброй ночи', 'добрый вечер', 'привет', 'спасибо',\n",
    "                  'добрый утро', 'добрый ночи', 'досвидание', 'до свидание', 'всего хорошего', 'всего хороший']\n",
    "    for word in stop_words:\n",
    "        pattern = r'' + word + ''\n",
    "        text = re.sub(pattern, '', text)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processing_phrases = []\n",
    "for _, value in dialogs_json.items():\n",
    "    phrases = value[\"visitors_phrases\"] + value[\"agents_phrases\"]\n",
    "    phrases = list(map(lambda x: remove_links(x), phrases))\n",
    "    phrases = list(map(lambda x: wifi_preprocessing(x), phrases))\n",
    "    phrases = list(map(lambda x: remove_punct_and_latin(x), phrases))\n",
    "    phrases = list(map(lambda x: replace_digits(x), phrases))\n",
    "    phrases = list(map(lambda x: x.lower(), phrases))\n",
    "    phrases = split_by_sentence(\".!?\", phrases)\n",
    "    phrases = list(map(lambda x: normalize(x), phrases))\n",
    "    phrases = list(map(lambda x: remove_stop_words(x), phrases))\n",
    "    phrases = [phrase.strip() for phrase in phrases if phrase.strip() !=\"\"]\n",
    "    all_processing_phrases += phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983896"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_processing_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/all_processing_phrases.txt\", mode=\"w\") as file:\n",
    "    for phrase in all_processing_phrases:\n",
    "        print(phrase, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, quantiles\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for phrase in all_processing_phrases:\n",
    "    lens.append(len(phrase.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9290799027539496"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 3.0, 6.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See to the sentences length destribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0, 234766, 178843, 111804, 123173,  88056,  83683,  52917,\n",
       "        39364,  21908,  13754,  11007,   6875,   4174,   4267,   2335,\n",
       "         2560,   1090,   1133,    496,    331,    211,    203,    221,\n",
       "          232,     69,     74,     94,     84,     30,     29,     15,\n",
       "           18,      5,      8,      9,      4,     13,      6,      1,\n",
       "            0,      6,      2,      5,      4,      0,      1,      1,\n",
       "            1,      0,      2,      0,      1,      0,      1,      0,\n",
       "            0,      0,      0,      0,      0,      0,      1,      1,\n",
       "            1,      0,      0,      0,      2,      0,      0,      0,\n",
       "            0,      0,      0,      1,      1,      1,      0,      0,\n",
       "            0,      0,      0,      2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most single word sentences ! This sentences meaningless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove phrase with length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_phrases_more_one_word = []\n",
    "for phrase in all_processing_phrases:\n",
    "    if len(phrase.split()) > 1:\n",
    "        processing_phrases_more_one_word.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749130"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processing_phrases_more_one_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/processing_phrases_more_one_word.txt\", mode=\"w\") as file:\n",
    "    for phrase in processing_phrases_more_one_word:\n",
    "        print(phrase, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
